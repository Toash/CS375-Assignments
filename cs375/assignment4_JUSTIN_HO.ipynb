{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tajfsk_7JY3E"
   },
   "source": [
    "**Note to grader:** Each question is assigned with a score. The final score will be (sum of actual scores)/(sum of maximum scores)*100. The grading rubrics are shown in the TA guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SArgW_Vq-uTh"
   },
   "source": [
    "# **Assignment 4**\n",
    "\n",
    "The goal of this assignment is to run some experiments with scikit-learn on a fairly sizeable and interesting image data set. This is the MNIST data set that consists of lots of images, each having 28x28 pixels. By today's standards, this may seem relatively tiny, but only a few years ago was quite challenging computationally, and it motivated the development of several ML algorithms and models that are now state-of-the-art  solutions for much bigger data sets. \n",
    "\n",
    "The assignment is experimental. We will try to whether a combination of PCA and kNN can yield any good results for the MNIST data set. Let's see if it can be made to work on this data set. \n",
    "\n",
    "Note: There are less difficult Python parts in this assignment. You can get things done by just repeating things from the class notebooks. But your participation and interaction via Canvas is always appreciated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFM4hig-uTj"
   },
   "source": [
    "## Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "E3alYkjM-uTk"
   },
   "outputs": [],
   "source": [
    "# Import all necessary python packages\n",
    "import numpy as np\n",
    "#import os\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "4JEBC9tZEZel"
   },
   "outputs": [],
   "source": [
    "# we load the data set directly from scikit learn \n",
    "# \n",
    "# note: this operation may take a few seconds. If for any reason it fails we \n",
    "# can revert back to loading from local storage. \n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = y.astype(int)\n",
    "X = ((X / 255.) - .5) * 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfrfDK0P-uT5"
   },
   "source": [
    "## <font color = 'blue'> Question 1. Inspecting the Dataset （50 pts = 10 pts by 5 questions)</font>\n",
    "\n",
    "**(i)** How many data points are in the training and test sets ? <br>\n",
    "**(ii)** How many attributes does the data set have ?\n",
    "\n",
    "Exlain how you found the answer to the first two questions. \n",
    "\n",
    "[**Hint**: Use the 'shape' method associated with numpy arrays. ]\n",
    "\n",
    "**(iii)** How many different labels does this data set have. Can you demonsrate how to read that number from the vector of labels *y_train*?  <br>\n",
    "**(iv)** How does the number of attributes relates to the size of the images? <br>\n",
    "**(v)** What is the role of line 12 (X = ((X / 255.) - .5) * 2) in the above code? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*(Please insert cells below for your answers. Clearly id the part of the question you answer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4po5m-tq-uT6"
   },
   "source": [
    "_____________________________________________________\n",
    "i: \n",
    "60000 data points in training set\n",
    "10000 data points in test set\n",
    "\n",
    "ii:\n",
    "784 attributes, another one for the output\n",
    "Used numpy shape function.\n",
    "_____________________________________________________\n",
    "\n",
    "_____________________________________________________\n",
    "iii:\n",
    "The dataset has 10 different labels- one for each digit.\n",
    "The label corresponds to the number it represents. (If label is 0, it represents the number zero).\n",
    "_____________________________________________________\n",
    "\n",
    "_____________________________________________________\n",
    "iv:\n",
    "The images are 28 by 28 pixels. 28 times 28 is 784, so each feature represents a single pixel.\n",
    "_____________________________________________________\n",
    "_____________________________________________________\n",
    "v: Each individual pixel is represented by numbers from 0 to 255, that code changes the range to -1 to 1 instead. Which makes it more convenient to apply machine learning algorithms.\n",
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "IllLoXxGAIIo"
   },
   "outputs": [],
   "source": [
    "# For grader use only\n",
    "maxScore = 0\n",
    "maxScore = maxScore + 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> Question 2. PCA on MNIST (10 pts) </font>\n",
    "\n",
    "Because the number of attributes of the MNIST data set may be too big to apply kNN on it (due to the 'curse of dimensionality'), we want to compress the images down to a smaller number of 'fake' attributes. \n",
    "\n",
    "Use scikit-learn to output a data set *X_train_transformed* and *X_test_transformed*, with $l$ attributes. Here a reasonable choice of $l$ is 10, equal to the number of labels. But you can try slightly smaller or bigger values as well. \n",
    "\n",
    "Print out the shape of *X_train_transformed* and *X_test_transformed*.\n",
    "\n",
    "\n",
    "**Hint**: Take a look at [this notebook](https://colab.research.google.com/drive/1DG5PjWejo8F7AhozHxj8329SuMtXZ874?usp=drive_fs), and imitate what we did there. Be careful though, to use only the scikit-learn demonstration, not the exhaustive PCA steps. \n",
    "\n",
    "**Note**: This computation can take a while. If problems are encountered we can try the same experiment on a downsized data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "a0ek4Ry_-uT_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transformed training dataset: (60000, 10)\n",
      "Shape of transformed test dataset: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mean of 0 std of 1\n",
    "sc = StandardScaler(); \n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Eigendecomposition of covariance matrix\n",
    "# Get direction and magnitude of principal \n",
    "#           components of data.\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "# eigen_vecs are the direction of principal components\n",
    "# eigan_vals are the magnitude of variance.\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "# eigenvalue, eigenvector tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "\n",
    "# sort the tuples from high to low\n",
    "# The first pair represents the principle component \n",
    "#         that represents the most variance in the data.\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# Choose the two top eigen pairs\n",
    "\"\"\"\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "\"\"\"\n",
    "\n",
    "# Top ten eigen pairs\n",
    "w = np.hstack([eigen_pairs[i][1][:, np.newaxis] for i in range(10)])\n",
    "\n",
    "X_train_transformed = np.dot(X_train_std,w)\n",
    "X_test_transformed = np.dot(X_test_std,w)\n",
    "\n",
    "print(f\"Shape of transformed training dataset: {np.shape(X_train_transformed)}\")\n",
    "print(f\"Shape of transformed test dataset: {np.shape(X_test_transformed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "sBjMZF1wGaUp"
   },
   "outputs": [],
   "source": [
    "# for grader use\n",
    "maxScore = maxScore + 10 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe9kKR3J-uUA"
   },
   "source": [
    "## <font color = 'blue'> Question 3. kNN on MNIST attributes from PCA （40 pts = 10 pts by 4 questions) </font>\n",
    "\n",
    "\n",
    "Having calculated the *transformed* MNIST data set we can now apply a kNN approach to the MNIST classification data set. Here are the sets:\n",
    "\n",
    "(i) Fit a $k$-NN classifier on the transformed data set. Here $k$ is a hyperparameter, and you can experiment with it. Be aware though, that larger $k$ can take more time to fit. \n",
    "\n",
    "(ii) Apply the classifier on the transformed test set. What is the classification accuracy? \n",
    "\n",
    "(iii) A theoretical question: if we skipped all the above steps and we just assigned a **random** label to each test point, what would the classification accuracy be on average?  Does your result (ii) beat the random expectation? (conduct 1000 trials to get the average accuracy)\n",
    "\n",
    "(iv) Experiment with different settings of $k$. Experiment design: calculates accuracy for increasing values of k; stops when k decreases for 5 values of k; report your findings in a separate cell.\n",
    "\n",
    "[**Hint**: Take a look at this [notebook](https://colab.research.google.com/drive/1Mh6I3bR8pE90kcs28JfKok59NtfV_7ct?usp=drive_fs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "5v7Q2NKp-uUM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.913\n",
      "Random Classification Accuracy: 0.1054\n",
      "Classification Accuracy for k value 1: 0.8956\n",
      "Classification Accuracy for k value 51: 0.9033\n",
      "Classification Accuracy for k value 101: 0.8913\n",
      "Classification Accuracy for k value 151: 0.8849\n",
      "Classification Accuracy for k value 201: 0.8801\n",
      "Classification Accuracy for k value 251: 0.876\n",
      "Classification Accuracy for k value 301: 0.8736\n"
     ]
    }
   ],
   "source": [
    "# i.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_transformed,y_train)\n",
    "\n",
    "# ii.\n",
    "# Use k nearest neighbors to predict the reduced dimensionality dataset.\n",
    "y_pred = knn.predict(X_test_transformed)\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "# The classification accuracy is 0.913.\n",
    "\n",
    "# iii.\n",
    "# set random label to each test point\n",
    "y_random  = np.random.randint(0, 10, size=10000)\n",
    "random_accuracy = accuracy_score(y_test,y_random)\n",
    "print(f\"Random Classification Accuracy: {random_accuracy}\")\n",
    "\n",
    "\"\"\"\n",
    "Since there are 10 possible labels, if we just randomly assigned labels the accuracy should be around 0.1, since there is a 1/10 chance to randomly guess one of the 10 labels.\n",
    "The classification accuracy beats the random classification accuracy by approximately 80 percent.\n",
    "\"\"\"\n",
    "\n",
    "# iv.\n",
    "#  Experiment design: calculates accuracy for increasing values of k; stops when k decreases for 5 values of k\n",
    "for k in range(1,302,50):\n",
    "    knn =  KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_transformed,y_train)\n",
    "\n",
    "    y_pred = knn.predict(X_test_transformed)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    print(f\"Classification Accuracy for k value {k}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy increases from 1 to 51, but decreases with 50 unit long increments. This is likely because the model is becoming more and more general, and missing small flucuations in data. (Underfitting is occuring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "NuYcPkCaGe9p"
   },
   "outputs": [],
   "source": [
    "# for grader use\n",
    "maxScore = maxScore + 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "fydXo8GRGkbp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actualScore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for grader use\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mactualScore\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m/\u001b[39mmaxScore\n",
      "\u001b[0;31mNameError\u001b[0m: name 'actualScore' is not defined"
     ]
    }
   ],
   "source": [
    "# for grader use\n",
    "\n",
    "score = actualScore*100/maxScore"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
